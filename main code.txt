import pandas as pd
import numpy as np
import os
import kagglehub
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, classification_report, roc_auc_score, roc_curve,
                             confusion_matrix, ConfusionMatrixDisplay)
from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import seaborn as sns
import shap

# Set random seed for reproducibility
np.random.seed(42)

# --- Download Dataset ---
try:
    path = kagglehub.dataset_download("brsahan/genomic-data-for-cancer")
except Exception as e:
    print("Error downloading dataset:", e)
    exit()

data_file = os.path.join(path, [f for f in os.listdir(path) if f.endswith('.csv')][0])
df = pd.read_csv(data_file)

# --- Preprocessing ---
X = df[['Gene One', 'Gene Two']].values
y = df['Cancer Present'].values

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False)
X_scaled_poly = poly.fit_transform(X_scaled)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled_poly, y, test_size=0.2, random_state=42)
print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

# Check class distribution
print("Class distribution in training set:", np.bincount(y_train))

# --- Data Visualizations ---
plt.figure(figsize=(10, 5))
for label in [0, 1]:
    sns.kdeplot(df[df['Cancer Present'] == label]['Gene One'], label=f'Cancer Present={label}', alpha=0.5)
plt.xlabel('Gene One Expression')
plt.ylabel('Density')
plt.title('Distribution of Gene One Expression')
plt.legend()
plt.savefig('gene_one_distribution.png')
plt.show()

plt.figure(figsize=(10, 5))
for label in [0, 1]:
    sns.kdeplot(df[df['Cancer Present'] == label]['Gene Two'], label=f'Cancer Present={label}', alpha=0.5)
plt.xlabel('Gene Two Expression')
plt.ylabel('Density')
plt.title('Distribution of Gene Two Expression')
plt.legend()
plt.savefig('gene_two_distribution.png')
plt.show()

# Scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(df['Gene One'], df['Gene Two'], c=df['Cancer Present'], cmap='bwr', alpha=0.5)
plt.xlabel('Gene One Expression')
plt.ylabel('Gene Two Expression')
plt.title('Gene One vs. Gene Two by Cancer Presence')
plt.colorbar(label='Cancer Present (0: No, 1: Yes)')
plt.savefig('gene_scatter.png')
plt.show()

# Correlation heatmap
correlation = df[['Gene One', 'Gene Two']].corr()
plt.figure(figsize=(6, 5))
sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Between Gene One and Gene Two')
plt.savefig('gene_correlation.png')
plt.show()
print("Correlation between Gene One and Gene Two:", correlation.iloc[0, 1])

# --- Neural Network Model ---
nn_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

nn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
                 loss='binary_crossentropy', metrics=['accuracy'])

nn_model.fit(X_train, y_train, epochs=180, batch_size=32,
             validation_data=(X_test, y_test),
             callbacks=[EarlyStopping(patience=20, restore_best_weights=True)],
             verbose=1)

# Evaluate Neural Network
loss, acc_nn = nn_model.evaluate(X_test, y_test)
print(f"Neural Network Accuracy: {acc_nn:.4f}")
y_pred_nn = (nn_model.predict(X_test) > 0.5).astype(int).flatten()
print("Neural Network Classification Report:")
print(classification_report(y_test, y_pred_nn))
auc_nn = roc_auc_score(y_test, nn_model.predict(X_test))
print(f"Neural Network AUC-ROC: {auc_nn:.4f}")

# --- Voting Classifier (SVM + XGBoost) ---
svm = SVC(kernel='rbf', probability=True, C=100, gamma=0.01)
xgb = XGBClassifier(learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.9)
voting_clf = VotingClassifier([('svm', svm), ('xgb', xgb)], voting='soft')
voting_clf.fit(X_train, y_train)

y_pred_voting = voting_clf.predict(X_test)
acc_voting = accuracy_score(y_test, y_pred_voting)
print(f"Voting Classifier Accuracy: {acc_voting:.4f}")
print("Voting Classifier Classification Report:")
print(classification_report(y_test, y_pred_voting))
auc_voting = roc_auc_score(y_test, voting_clf.predict_proba(X_test)[:, 1])
print(f"Voting Classifier AUC-ROC: {auc_voting:.4f}")

# --- Stacking Classifier ---
stacking_clf = StackingClassifier(
    estimators=[('voting', voting_clf)],
    final_estimator=LogisticRegression(),
    cv=5
)
stacking_clf.fit(X_train, y_train)

y_pred_stacking = stacking_clf.predict(X_test)
acc_stacking = accuracy_score(y_test, y_pred_stacking)
print(f"Stacking Classifier Accuracy: {acc_stacking:.4f}")
print("Stacking Classifier Classification Report:")
print(classification_report(y_test, y_pred_stacking))
auc_stacking = roc_auc_score(y_test, stacking_clf.predict_proba(X_test)[:, 1])
print(f"Stacking Classifier AUC-ROC: {auc_stacking:.4f}")

# --- Confusion Matrices ---
for model_name, y_pred in zip(['Neural Network', 'Voting Classifier', 'Stacking Classifier'],
                              [y_pred_nn, y_pred_voting, y_pred_stacking]):
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot()
    plt.title(f'Confusion Matrix: {model_name}')
    plt.savefig(f'confusion_matrix_{model_name.replace(" ", "_").lower()}.png')
    plt.show()

# --- ROC Curves ---
fpr_nn, tpr_nn, _ = roc_curve(y_test, nn_model.predict(X_test))
fpr_voting, tpr_voting, _ = roc_curve(y_test, voting_clf.predict_proba(X_test)[:, 1])
fpr_stacking, tpr_stacking, _ = roc_curve(y_test, stacking_clf.predict_proba(X_test)[:, 1])

plt.figure(figsize=(8, 6))
plt.plot(fpr_nn, tpr_nn, label=f'Neural Network (AUC = {auc_nn:.2f})')
plt.plot(fpr_voting, tpr_voting, label=f'Voting Classifier (AUC = {auc_voting:.2f})')
plt.plot(fpr_stacking, tpr_stacking, label=f'Stacking Classifier (AUC = {auc_stacking:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.legend()
plt.savefig('roc_curves.png')
plt.show()

# --- SHAP Feature Importance (XGBoost inside VotingClassifier) ---
explainer = shap.TreeExplainer(voting_clf.named_estimators_['xgb'])
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, feature_names=poly.get_feature_names_out(['Gene One', 'Gene Two']),
                  show=False)
plt.savefig('shap_summary.png')
plt.show()
